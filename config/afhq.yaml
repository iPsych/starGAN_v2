# logger options
log_iter: 50                   # How often do you want to log the training stats
image_display_iter: 200        # How often do you want to display output images during training
image_save_iter: 500          # How often do you want to save output images during training

# optimization options
max_iter: 100000               # maximum number of training iterations
batch_size: 8                  # batch size
batch_size_test: 4             # batch size for test samples
optimizer: Adam                # optimizer
beta1: 0                       # Adam parameter
beta2: 0.99                    # Adam parameter
init: kaiming                  # initialization [gaussian/kaiming/xavier/orthogonal]
d_step: 1                      # how often to update dilosscriminator
g_step: 1                      # how often to update generator
lr: 0.0001                     # initial learning rate
lr_F: 0.000001                 # initial learning rate for mapping network
#lr_policy: step               # learning rate scheduler
#step_size: 1000               # how often to decay learning rate  # TODO : check Olr decay details
#init_decay_step: 5000         #
#gamma: 0.99999                # how much to decay learning rate
w_style: 0.3                   # weight of style reconstruction loss
w_ds: 1                        # weight of style diversification loss
w_cyc: 0.1                     # weight of cycle consistency loss
w_regul: 1                     # weight of non-saturating gan R1 regularization term

# model options
gan_type: bce                  # GAN loss [bce/lsgan/wgan]
dim_style: 64                  # style dimension
gen:
  dim: 32                      # number of filters in the bottommost layer
  n_downs: 4                   # number of downsampling layers
  n_intermediates: 4           # number of intermediate layers
  activation: relu             # activation function [relu/lrelu/prelu/selu/tanh]
  pad_type: zero               # padding type [zero/reflect]
mapping_network:
  dim_latent: 16               # dimension of latent vector
  dim: 512                     # number of filters in the bottommost layer
  n_mlp: 6                     # number of mlp layers
  norm: none                   # normalization layer [none/bn/in/ln/adain/adain_affine]
  activation: relu             # activation function [relu/lrelu/prelu/selu/tanh]
  pad_type: zero               # padding type [zero/reflect]
style_encoder:
  ch: 16                       # channel multiplier
  n_intermediates: 6           # number of intermediate layers
  activation: relu             # activation function [relu/lrelu/prelu/selu/tanh]
  output_dim: 64                        # output dimension, same as style dimension
dis:
  ch: 32                       # channel multiplier
  n_intermediates: 6           # number of intermediate layers
  activation: lrelu            # activation function [relu/lrelu/prelu/selu/tanh]
  output_dim: 1                         # output dimension, 1 for binary classification

# data load options
is_flip: True                             # whether horizontally flipping images when reading images
num_workers: 8                            # number of data loading threads
re_size: 280                              # first resize the shortest image side to this size
crop_size: 256                            # random crop image of this height    # TODO : arbitrary
img_path: afhq                            # path of image saving
save_name: afhq                           # unique name for save folder
